---
title: "previsão_nao_supervisionada"
author: "Guilherme Araujo Gonçalves de Souza"
date: "25 de fevereiro de 2018"
output: html_document
---

# Predição/previsão não supervisionada

### Ideias chave
* Para construir um preditor:
* 1. Crie clusters
* 2. Nomeie clusters
* 3. Construir preditores para clusters

* Num novo conjunto de dados
* - Previsão de clusters

```{r}
#Carregando pacotes
suppressMessages(library(caret))
suppressMessages(library(ElemStatLearn))
suppressMessages(library(tidyverse))
suppressMessages(library(magrittr))
suppressMessages(library(ISLR))
suppressMessages(library(quantmod))
suppressMessages(library(elasticnet))
suppressMessages(library(lubridate))
```

```{r}
data(iris)
inTrain <- caret::createDataPartition(y = iris$Species, p = 0.7, list = FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
```


### Cluster com K-means
```{r}
Kmeans1 <- kmeans(x = training %>% 
                    select(-Species), centers = 3)
training$clusters <- as.factor(Kmeans1$cluster)
training %>% 
  ggplot(aes(x = Petal.Width, y = Petal.Length, colour = clusters)) + geom_point() + theme_bw()

#Comparando com a real classificação
table(Kmeans1$cluster, training$Species)
```

#### Construindo um preditor
```{r}
modFit <- caret::train(clusters ~., data = training %>% select(-Species), method = "rpart")
table(predict(modFit, training), training$Species)
```

```{r}
#Aplicando em teste
clusterPred <- predict(modFit, testing)
table(clusterPred, testing$Species)
```

### Random forest e Gradient boosting model / Ensemble dos dois
```{r}
set.seed(33833)
training <- ElemStatLearn::vowel.train
testing <- ElemStatLearn::vowel.test
training$y %<>% as.factor()
testing$y %<>% as.factor()

modRF <- caret::train(y ~., method = "rf", data = training, verbose = FALSE)
modGBM <- caret::train(y ~., method = "gbm", data = training)
```

```{r}
#Previsão
set.seed(33833)
predmodRF <- predict(modRF, testing)
predmodGBM <- predict(modGBM, testing)
```

```{r}
#Matriz de confusão
set.seed(33833)
confusionMatrix(predmodRF, testing$y)$overall[1]

confusionMatrix(predmodGBM, testing$y)$overall[1]
```

```{r}
predDF <- data.frame(predmodRF, predmodGBM, y = testing$y)

# Acurácia entre o amostra de teste onde os métodos concordam
sum(predmodRF[predDF$predmodRF == predDF$predmodGBM] == 
        predDF$y[predDF$predmodRF == predDF$predmodGBM]) / 
    sum(predDF$predmodRF == predDF$predmodGBM)
```


#### Random forest, GBM e Análise discriminante / Ensemble dos 3
```{r}
set.seed(3433)

library(AppliedPredictiveModeling)

data(AlzheimerDisease)

adData = data.frame(diagnosis,predictors)

inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]

training = adData[ inTrain,]

testing = adData[-inTrain,]
```

```{r}
set.seed(62433)
modRF <- caret::train(diagnosis ~., method = "rf", data = training)

modgbm <- caret::train(diagnosis ~., method = "gbm", data = training)

modlda <- caret::train(diagnosis ~., method = "lda", data = training)
```

```{r}
set.seed(62433)
predRF <- predict(modRF, testing)
predgbm <- predict(modgbm, testing)
predlda <- predict(modlda, testing)

#Acurácia Random forest
confusionMatrix(predRF, testing$diagnosis)$overall[1]

#Acurácia gbm
confusionMatrix(predgbm, testing$diagnosis)$overall[1]

#Acurácia lda
confusionMatrix(predlda, testing$diagnosis)$overall[1]
```

```{r}
predDF <- data.frame(predRF, predgbm, predlda, diagnosis = testing$diagnosis)
combMod <- caret::train(diagnosis ~., data = predDF, method = "rf")
combpred <- predict(combMod, predDF)
```

#### Matriz de confusão
```{r}
confusionMatrix(predRF, testing$diagnosis)$overall[1]

confusionMatrix(predgbm, testing$diagnosis)$overall[1]

confusionMatrix(predlda, testing$diagnosis)$overall[1]

confusionMatrix(combpred, testing$diagnosis)$overall[1]
```


# Regressão regularizada por (Lasso)
```{r}
set.seed(3523)

library(AppliedPredictiveModeling)

data(concrete)

inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]

training = concrete[ inTrain,]

testing = concrete[-inTrain,]

```

```{r}
#Ajustando Lasso
modlasso <- caret::train(CompressiveStrength ~., method = "lasso", data = training)

#Qual variável é o último coeficiente a ser definido como zero à medida que a penalidade aumenta?
elasticnet::plot.enet(modlasso$finalModel, xvar = "penalty", use.color = TRUE)
```

```{r}
dat = read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv")

training = dat[year(dat$date) < 2012,]

testing = dat[(year(dat$date)) > 2011,]

tstrain = ts(training$visitsTumblr)
```

```{r}
library(forecast)
mod_ts <- bats(tstrain)
fcast <- forecast(mod_ts, level = 95, h = dim(testing)[1])
sum(fcast$lower < testing$visitsTumblr & testing$visitsTumblr < fcast$upper) / 
    dim(testing)[1]
```

# Support vector machine
```{r}
set.seed(3523)

library(AppliedPredictiveModeling)

data(concrete)

inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]

training = concrete[ inTrain,]

testing = concrete[-inTrain,]
```

```{r}
set.seed(325)
library(e1071)
mod_svm <- svm(CompressiveStrength ~ ., data = training)
pred_svm <- predict(mod_svm, testing)
rmse_svm <- sqrt(mean((testing$CompressiveStrength - pred_svm)^2))
rmse_svm

```
